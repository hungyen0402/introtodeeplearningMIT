# MIT 6.S191: L·ªùi gi·∫£i v√† Th·ª±c h√†nh Deep Learning
## Portfolio c√° nh√¢n c·ªßa Ho√†ng ƒê·ª©c H∆∞ng

Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi kho l∆∞u tr·ªØ (repository) ghi l·∫°i qu√° tr√¨nh h·ªçc t·∫≠p v√† ho√†n th√†nh c√°c b√†i lab c·ªßa t√¥i t·ª´ kh√≥a h·ªçc uy t√≠n **MIT 6.S191: Introduction to Deep Learning**.

Repo n√†y kh√¥ng ch·ªâ l√† m·ªôt b·∫£n "fork" ƒë∆°n thu·∫ßn. T√¥i s·ª≠ d·ª•ng n√≥ l√†m m·ªôt portfolio k·ªπ thu·∫≠t, n∆°i t√¥i t·ª± tay l·∫≠p tr√¨nh (code) c√°c gi·∫£i ph√°p, ho√†n th√†nh c√°c ph·∫ßn `TODO`, v√† l∆∞u l·∫°i k·∫øt qu·∫£ ƒë·∫ßu ra c·ªßa m√¨nh. M·ª•c ti√™u c·ªßa t√¥i l√† x√¢y d·ª±ng s·ª± hi·ªÉu bi·∫øt s√¢u s·∫Øc v·ªÅ c√°c m√¥ h√¨nh deep learning t·ª´ nh·ªØng nguy√™n l√Ω c∆° b·∫£n nh·∫•t.

> **Repo g·ªëc c·ªßa kh√≥a h·ªçc:** ƒê√¢y l√† b·∫£n fork t·ª´ kho l∆∞u tr·ªØ ch√≠nh th·ª©c [MITDeepLearning/introtodeeplearning](https://github.com/MITDeepLearning/introtodeeplearning). To√†n b·ªô t√†i li·ªáu g·ªëc v√† ƒë·ªÅ b√†i thu·ªôc b·∫£n quy·ªÅn c·ªßa ƒë·ªôi ng≈© gi·∫£ng vi√™n MIT 6.S191.

---

## üöÄ C√°c k·ªπ nƒÉng v√† kh√°i ni·ªám ƒë√£ th·ª±c h√†nh

Qua c√°c b√†i lab n√†y, t√¥i ƒë√£ tr·ª±c ti·∫øp x√¢y d·ª±ng v√† hu·∫•n luy·ªán c√°c m√¥ h√¨nh, qua ƒë√≥ n·∫Øm v·ªØng c√°c k·ªπ nƒÉng:

* **PyTorch (N·ªÅn t·∫£ng):** X√¢y d·ª±ng c√°c layer m·∫°ng n∆°-ron t√πy ch·ªânh (custom `nn.Module`), hi·ªÉu r√µ v·ªÅ tensors, v√† v√≤ng l·∫∑p hu·∫•n luy·ªán.
* **Automatic Differentiation:** V·∫≠n d·ª•ng `torch.autograd` v√† `loss.backward()` ƒë·ªÉ tri·ªÉn khai thu·∫≠t to√°n Gradient Descent, t·ªëi ∆∞u h√≥a c√°c tham s·ªë.
* **M·∫°ng N∆°-ron T√≠ch ch·∫≠p (CNNs):** X√¢y d·ª±ng c√°c m√¥ h√¨nh CNN ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n th·ªã gi√°c m√°y t√≠nh (computer vision).
* **M·∫°ng N∆°-ron H·ªìi quy (RNNs):** Hi·ªÉu v√† x√¢y d·ª±ng m·∫°ng RNN t·ª´ ƒë·∫ßu ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu tu·∫ßn t·ª± (sequential data).
* **M√¥ h√¨nh Sinh nh·∫°c (Generative Models):** Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh RNN ƒë·ªÉ t·ª± ƒë·ªông s√°ng t√°c c√°c b·∫£n nh·∫°c m·ªõi.
* **Attention & Transformers:** (S·∫Ω c·∫≠p nh·∫≠t) Tri·ªÉn khai c∆° ch·∫ø self-attention, n·ªÅn t·∫£ng c·ªßa c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) hi·ªán ƒë·∫°i.

---

## üìÇ T·ªïng quan c√°c b√†i Lab ƒë√£ ho√†n th√†nh

D∆∞·ªõi ƒë√¢y l√† t√≥m t·∫Øt c√°c b√†i lab t√¥i ƒë√£ ho√†n th√†nh v√† nh·ªØng g√¨ t√¥i ƒë√£ l√†m trong m·ªói b√†i.

### üìù Lab 1: Gi·ªõi thi·ªáu PyTorch & S√°ng t√°c Nh·∫°c v·ªõi RNN

Trong b√†i lab n√†y, t√¥i ƒë√£ x√¢y d·ª±ng c√°c th√†nh ph·∫ßn c·ªët l√µi c·ªßa deep learning t·ª´ con s·ªë kh√¥ng v√† √°p d·ª•ng ch√∫ng v√†o m·ªôt nhi·ªám v·ª• s√°ng t·∫°o.

* **Ph·∫ßn 1: Gi·ªõi thi·ªáu PyTorch**
    * T·ª± tay x√¢y d·ª±ng m·ªôt layer `OurDenseLayer` (fully-connected) b·∫±ng c√°ch k·∫ø th·ª´a `nn.Module`.
    * Th·ª±c h√†nh c√°c ph√©p to√°n tensor v√† hi·ªÉu c√°ch PyTorch qu·∫£n l√Ω tham s·ªë (`nn.Parameter`).
    * Tri·ªÉn khai v√≤ng l·∫∑p **Gradient Descent** th·ªß c√¥ng ƒë·ªÉ t·ªëi ∆∞u h√†m loss $L=(x-x_f)^2$, qua ƒë√≥ hi·ªÉu r√µ c√°ch `loss.backward()` v√† `x.grad` ho·∫°t ƒë·ªông.

* **Ph·∫ßn 2: S√°ng t√°c Nh·∫°c v·ªõi RNN**
    * X√¢y d·ª±ng m√¥ h√¨nh **RNN** t√πy ch·ªânh, bao g·ªìm c·∫£ h√†m `forward` ƒë·ªÉ x·ª≠ l√Ω tr·∫°ng th√°i ·∫©n (hidden state) qua t·ª´ng b∆∞·ªõc th·ªùi gian (time step).
    * Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n kho d·ªØ li·ªáu nh·∫°c ABC notation.
    * S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ **sinh ra c√°c b·∫£n nh·∫°c m·ªõi** b·∫±ng c√°ch d·ª± ƒëo√°n chu·ªói n·ªët nh·∫°c ti·∫øp theo.

### üñºÔ∏è Lab 2: Th·ªã gi√°c M√°y t√≠nh (Computer Vision)

*(...ƒêang th·ª±c hi·ªán...) - T√¥i s·∫Ω c·∫≠p nh·∫≠t ph·∫ßn n√†y sau khi ho√†n th√†nh.*

### ü§ñ Lab 3: M·∫°ng Transformers v√† M√¥ h√¨nh Ng√¥n ng·ªØ

*(...S·∫Øp t·ªõi...) - T√¥i s·∫Ω c·∫≠p nh·∫≠t ph·∫ßn n√†y sau khi ho√†n th√†nh.*

---

## üõ†Ô∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng

* **Ng√¥n ng·ªØ:** Python
* **Th∆∞ vi·ªán Deep Learning:** PyTorch
* **M√¥i tr∆∞·ªùng:** Google Colab (s·ª≠ d·ª•ng T4 GPU)
* **C√¥ng c·ª• kh√°c:** NumPy, Matplotlib, Git
